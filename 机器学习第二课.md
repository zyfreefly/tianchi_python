0 贝叶斯公式
=========
朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，  
也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数Y=f(X),要么是条件分布P(Y|X)。  
但是朴素贝叶斯却是生成方法，也就是直接找出特征输出Y和特征X的联合分布P(X,Y),然后用P(Y|X)=P(X,Y)/P(X)得出。  
朴素贝叶斯很直观，计算量也不大，在很多领域有广泛的应用。

贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。

我们先看看条件独立公式，如果X和Y相互独立，则有：P(A,B)=P(A)P(B)

我们接着看看条件概率公式：P(AB)=P(A|B)P(B)=P(B|A)P(A)
全概率公式：
贝叶斯公式：

1 sklearn中的代码实现
============

在scikit-learn中，常见的有3个朴素贝叶斯的分类算法类。分别是GaussianNB，MultinomialNB和BernoulliNB。
其中GaussianNB就是先验为高斯分布的朴素贝叶斯，MultinomialNB就是先验为多项式分布的朴素贝叶斯，而BernoulliNB就是先验为伯努利分布的朴素贝叶斯。
最新的sklearn新增了Complement Naive Bayes和 Categorical Naive Bayes的算法实现，对应的类分别是ComplementNB 和CategoricalNB 

···
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> Y = np.array([1, 1, 1, 2, 2, 2])
>>> from sklearn.naive_bayes import GaussianNB
>>> clf = GaussianNB()
>>> clf.fit(X, Y)
GaussianNB()
>>> print(clf.predict([[-0.8, -1]]))
[1]
>>> clf_pf = GaussianNB()
>>> clf_pf.partial_fit(X, Y, np.unique(Y))
GaussianNB()
>>> print(clf_pf.predict([[-0.8, -1]]))
[1]
···
