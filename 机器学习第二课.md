0 贝叶斯公式
=========
朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，  
也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数Y=f(X),要么是条件分布P(Y|X)。  
但是朴素贝叶斯却是生成方法，也就是直接找出特征输出Y和特征X的联合分布P(X,Y),然后用P(Y|X)=P(X,Y)/P(X)得出。  
朴素贝叶斯很直观，计算量也不大，在很多领域有广泛的应用。

贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。

我们先看看条件独立公式，如果X和Y相互独立，则有：P(A,B)=P(A)P(B)

我们接着看看条件概率公式：P(AB)=P(A|B)P(B)=P(B|A)P(A)  或者  
![1](http://latex.codecogs.com/svg.latex?P(B%7CA)%20=%20%5Cfrac%7BP(AB)%7D%7BP(A)%7D)  
根据条件概率公式可以概率的乘法公式： P(AB) = P(A)P(B|A) = P(B)P(A|B)  

全概率公式：

![2](http://latex.codecogs.com/svg.latex?P(A)%20=%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20P(A%20B_i)%20=%20%5Csum_%7Bi=1%7D%5E%7Bn%7DP(B_i)P(A%7CB_i))

贝叶斯公式：  

![3](http://latex.codecogs.com/svg.latex?P(A%7CB)%20=%20%5Cfrac%7BP(B%7CA)%5Ccdot%20P(A)%7D%7BP(B)%7D)

- P(A|B) 是已知B发生后A的**条件概率**，也由于得自B的取值而被称作**A的后验概率**。  

- P(B|A) 是已知A发生后B的**条件概率**，也由于得自A的取值而被称作**B的后验概率**。    

- P(A)是**A的先验概率**(或边缘概率)。之所以称为“先验”，是因为它不用考虑任何B方面的因素。  
- P(B)是**B的先验概率**(或边缘概率)。  

机器学习中贝叶斯分类器的核心就是在已知特征X的情况下，计算样品属于某个类别Ci的概率，因此这个条件概率可以表示为：
![4](http://latex.codecogs.com/svg.latex?P(C_i%7CX)%20=%20%5Cfrac%7BP(C_%7Bi%7DX)%7D%7BP(X)%7D%20=%20%5Cfrac%7BP(C_i)P(X%7CC_i)%7D%7B%5Csum_%7Bi=1%7D%5E%7Bk%7DP(C_i)P(X%7CC_i)%7D)    



1 sklearn中的代码实现
============

在scikit-learn中，常见的有3个朴素贝叶斯的分类算法类。分别是GaussianNB，MultinomialNB和BernoulliNB。
其中GaussianNB就是先验为高斯分布的朴素贝叶斯，MultinomialNB就是先验为多项式分布的朴素贝叶斯，而BernoulliNB就是先验为伯努利分布的朴素贝叶斯。
最新的sklearn新增了Complement Naive Bayes和 Categorical Naive Bayes的算法实现，对应的类分别是ComplementNB 和CategoricalNB 

```python
>>> import numpy as np  
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])  
>>> Y = np.array([1, 1, 1, 2, 2, 2])  
>>> from sklearn.naive_bayes import GaussianNB  
>>> clf = GaussianNB()  
>>> clf.fit(X, Y)  
GaussianNB()  
>>> print(clf.predict([[-0.8, -1]]))  
[1]  
>>> clf_pf = GaussianNB()  
>>> clf_pf.partial_fit(X, Y, np.unique(Y))  
GaussianNB()  
>>> print(clf_pf.predict([[-0.8, -1]]))  
[1]  
```
