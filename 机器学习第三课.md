# 0 KNN介绍

KNN(k-nearest neighbors)，中文翻译K近邻，既可以用于分类算法，也可以用于回归算法。  
KNN的两个重要参数：邻居个数（一般使用较小的邻居个数，比如3个或5个），数据点之间距离的度量方法（默认使用欧式距离）。  
KNN的优点是模型很容易理解，缺点是数据量比较大时，预测速度较慢，不能处理特征较多的数据集。


**1) KNN建立过程**

    1. 给定测试样本，计算它与训练集中的每一个样本的距离。
    2. 找出距离近期的K个训练样本。作为测试样本的近邻。
    3. 依据这K个近邻归属的类别来确定样本的类别。


**2) 类别的判定**

- 投票决定，少数服从多数。取类别最多的为测试样本类别。

- 加权投票法，依据计算得出距离的远近，对近邻的投票进行加权，距离越近则权重越大，设定权重为距离平方的倒数。


# 1 sklearn代码实现

```python
# KNN分类
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import KNeighborsClassifier
>>> neigh = KNeighborsClassifier(n_neighbors=3)
>>> neigh.fit(X, y)
KNeighborsClassifier(...)
>>> print(neigh.predict([[1.1]]))
[0]
>>> print(neigh.predict_proba([[0.9]]))
[[0.66666667 0.33333333]]
```

```python
# KNN回归
>>> X = [[0], [1], [2], [3]]
>>> y = [0, 0, 1, 1]
>>> from sklearn.neighbors import KNeighborsRegressor
>>> neigh = KNeighborsRegressor(n_neighbors=2)
>>> neigh.fit(X, y)
KNeighborsRegressor(...)
>>> print(neigh.predict([[1.5]]))
[0.5]
```

```python
# KNNImputer缺失值填充
>>> import numpy as np
>>> from sklearn.impute import KNNImputer
>>> X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]
>>> imputer = KNNImputer(n_neighbors=2)
>>> imputer.fit_transform(X)
array([[1. , 2. , 4. ],
       [3. , 4. , 3. ],
       [5.5, 6. , 5. ],
       [8. , 8. , 7. ]])
```
